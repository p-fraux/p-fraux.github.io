
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. A propos de la parcimonie (H.P.) &#8212; Initiation aux statistiques - Notions et analyse statistique</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css?v=982b99e0" />
    <link rel="stylesheet" type="text/css" href="/Users/parent/opt/anaconda3/lib/python3.9/site-packages/sphinxcontrib/icon/node_modules/@fortawesome/fontawesome-free/css/all.min.css?v=53359634" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/mystyle.css?v=0f3ba4bc" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/.ipynb_checkpoints/mystyle-checkpoint.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="/Users/parent/opt/anaconda3/lib/python3.9/site-packages/sphinxcontrib/icon/node_modules/@fortawesome/fontawesome-free/js/all.min.js?v=ec35c6c9"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]]}, "svg": {"fontCache": "global"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ch6intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Annexes" href="ch7intro.html" />
    <link rel="prev" title="5.3. Tester la loi d’observations" href="ch5sec3.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/N7solo.png" class="logo__image only-light" alt="Initiation aux statistiques - Notions et analyse statistique - Home"/>
    <img src="_static/N7solo.png" class="logo__image only-dark pst-js-only" alt="Initiation aux statistiques - Notions et analyse statistique - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Bienvenue au cours de Statistiques
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="ch1intro.html">1. <i class="fas fa-book fa-fw"></i> Premières statistiques</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch1sec1.html">1.1. Motivations : Pour quoi faire des statistiques ?</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch1sec2.html">1.2. Les expériences statistiques et les statistiques</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch1sec3.html">1.3. Moyenne, variance et fonction de répartition empirique</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch1sec4.html">1.4. Lois Usuelles et leurs tabulations</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch2intro.html">2. <i class="fas fa-book fa-fw"></i> Estimateur : exemple de l’Estimateur de Maximum de Vraisemblance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch2sec1.html">2.1. Estimateur et premiers critères</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch2sec2.html">2.2. Efficacité d’un estimateur</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch2sec3.html">2.3. Estimateur de Maximum de Vraisemblance</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch2sec4.html">2.4. Amélioration théorique d’estimateurs (HP)</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch3intro.html">3. <i class="fas fa-book fa-fw"></i> Tests</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch3sec1.html">3.1. Hypothèses statistique et tests</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch3sec2.html">3.2. Erreurs et courbe ROC</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch3sec3.html">3.3. Test paramétrique simple</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch3sec4.html">3.4. Test paramétrique composite</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch3sec5.html">3.5. Tests de proportion</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch4intro.html">4. <i class="fas fa-book fa-fw"></i> Les modèles gaussiens</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch4sec1.html">4.1. Rappels sur la loi</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch4sec2.html">4.2. Théorème de Cochran (HP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch4sec3.html">4.3. Test d’égalité de variance et de moyenne</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch4sec4.html">4.4. Estimation par régression linéaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch4sec5.html">4.5. Test d’hypothèse linéaire</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch5intro.html">5. <i class="fas fa-book fa-fw"></i> Deux classes de tests non paramétrique</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch5sec1.html">5.1. Statistique de test du <span class="math notranslate nohighlight">\(\chi^2\)</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="ch5sec2.html">5.2. Test du <span class="math notranslate nohighlight">\(\chi^2\)</span> : test d’homogénéité et tests d’indépendance</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch5sec3.html">5.3. Tester la loi d’observations</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. <i class="fas fa-book fa-fw"></i>  A propos de la parcimonie (H.P.)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ch7intro.html">7. <i class="fas fa-book fa-fw"></i>  Annexes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="ch7sec1.html">7.1. Inverse généralisé de fonction de répartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch7sec2.html">7.2. Inégalités probabilistes</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch7sec3.html">7.3. Compléments d’algèbre linéaire</a></li>
<li class="toctree-l2"><a class="reference internal" href="ch7sec4.html">7.4. Convergence supplémentaire</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="biblio.html">8. <i class="fa-solid fa-book-open"></i> Bibliographie</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://p-fraux.github.io/Statistiques/intro.html" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://p-fraux.github.io/Statistiques/intro.html/issues/new?title=Issue%20on%20page%20%2Fch6intro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/ch6intro.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1><i class="fas fa-book fa-fw"></i>  A propos de la parcimonie (H.P.)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithmes-de-selection-de-parametres-pour-la-regression-lineaire">6.1. Algorithmes de sélection de paramètres pour la régression linéaire</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-analyse-en-composante-principale-pour-diminuer-la-dimension">6.2. L’analyse en composante principale pour diminuer la dimension</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="a-propos-de-la-parcimonie-h-p">
<h1><span class="section-number">6. </span><i class="fas fa-book fa-fw"></i>  A propos de la parcimonie (H.P.)<a class="headerlink" href="#a-propos-de-la-parcimonie-h-p" title="Link to this heading">#</a></h1>
<p><span class="math notranslate nohighlight">\(\newcommand{\R}{\mathbb{R}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\Q}{\mathbb{Q}}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\N}{\mathbb{N}}\)</span></p>
<div class="exercise dropdown admonition" id="exer18">

<p class="admonition-title"><span class="caption-number">Exercise 6.1 </span> ((H.P. du Hors Programme) Motivation avec un exemple : Régression non paramétrique - base de Fourrier - D’après le TD du Pr. Guyader)</p>
<section id="exercise-content">
<p>Nous nous intéressons à un problème plus difficile  : considérons que nous observons les variables aléatoires
<span class="math notranslate nohighlight">\(Y_i = f(\frac{i}{n}) + \epsilon_i\)</span> pour <span class="math notranslate nohighlight">\(i\in [\![1,n]\!]\)</span>, somme des valeurs d’une fonction avec un bruit. Nous supposons donc que les <span class="math notranslate nohighlight">\(\epsilon_i\)</span> sont i.i.d. de loi <span class="math notranslate nohighlight">\(\mathcal{N}(0,\sigma^2)\)</span> avec <span class="math notranslate nohighlight">\(\sigma\)</span> connu, et que <span class="math notranslate nohighlight">\(f : [0, 1] \to \R\)</span> est une fonction inconnue qui est le paramètre d’intérêt.</p>
<p>a) Quel est la difficulté particulière de ce modèle statistique par rapport à une régression classique ?</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution a)</p>
<p>Il ne s’agit pas d’un problème linéaire.</p>
</div>
<p><strong>Pour résoudre ce problème, on propose de projeter f sur une base de fonction bien choisie. Supposons pour la fin de l’exercice que</strong></p>
<div class="math notranslate nohighlight">
\[\forall t\in [0,1], f(t)=a_0+\sum_{k=1}^K a_k\cos(2k\pi t)+b_k\sin(2k\pi t)\]</div>
<p>Les inconnues deviennent alors  <span class="math notranslate nohighlight">\((a_i)_{0\leq i \leq K}\)</span> et <span class="math notranslate nohighlight">\((b_i)_{1\leq i\leq K}\)</span>.</p>
<p>b) Sous cette hypothèse, écrire le modèle comme un modèle linéaire gaussien <span class="math notranslate nohighlight">\(Y = X\beta + \epsilon\)</span> et préciser X, <span class="math notranslate nohighlight">\(\beta\)</span> et le nombre de variables explicatives p.</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution b)</p>
<p>Nous avons alors <span class="math notranslate nohighlight">\(p=2K+1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split} \beta= \begin{pmatrix}
            a_0\\a_1\\\vdots\\ a_K\\b_1\\\vdots\\b_n
        \end{pmatrix}\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split} X=\begin{pmatrix}
            1 &amp; \cos(2\pi\frac{1}{n}) &amp;\cdots &amp;\cos(2k\pi\frac{1}{n}) &amp;\cdots &amp;\cos(2K\pi\frac{1}{n})&amp; \sin(2\pi\frac{1}{n}) &amp;\cdots &amp;\sin(2 k\pi\frac{1}{n})&amp;\cdots &amp; \sin(2 K\pi\frac{1}{n}) \\ 
            \vdots &amp;\vdots&amp;&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;&amp;\vdots\\
            1 &amp; \cos(2\pi\frac{i}{n}) &amp;\cdots &amp;\cos(2k\pi\frac{i}{n}) &amp;\cdots &amp;\cos(2K\pi\frac{i}{n})&amp; \sin(2\pi\frac{i}{n}) &amp;\cdots &amp;\sin(2 k\pi\frac{i}{n})&amp;\cdots &amp; \sin(2 K\pi\frac{i}{n}) \\ 
            \vdots &amp;\vdots&amp;&amp;\vdots&amp;&amp;\vdots&amp;\vdots&amp;&amp;\vdots&amp;&amp;\vdots\\
            1 &amp; 1 &amp;\cdots &amp;1 &amp;\cdots &amp;1&amp; 0 &amp;\cdots &amp;0&amp;\cdots &amp; 0 \\ 
        \end{pmatrix}\end{split}\]</div>
</div>
<p>c)  On suppose à présent <span class="math notranslate nohighlight">\(2K + 1 \leq n\)</span>. Vérifier que le modèle est identifiable (que la matrice est de rang plein), et calculer l’estimateur des moindres carrés <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> de <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>En déduire un estimateur <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> de <span class="math notranslate nohighlight">\(\mu=[f(\frac{i}{n})]_{i\in [\![1,n]\!]}\)</span>, et proposer un estimateur <span class="math notranslate nohighlight">\(\hat{f}\)</span> de la fonction f.</p>
<p><em>On pourra se rappeler les formules de trigonométrie, en notant j une racine carré de <span class="math notranslate nohighlight">\(-1\)</span> :</em></p>
<div class="math notranslate nohighlight">
\[ \cos(A)\cos(B) = \frac{1}{2}Re(e^{j(A+B)}+e^{j(A-B)})\]</div>
<div class="math notranslate nohighlight">
\[ \cos(A)\sin(B) = \frac{1}{2}Im(e^{j(A+B)}-e^{j(A-B)})\]</div>
<div class="math notranslate nohighlight">
\[ \sin(A)\sin(B) = \frac{1}{2}Re(e^{j(A-B)}-e^{j(A+B)})\]</div>
<div class="dropdown admonition">
<p class="admonition-title">Solution c)</p>
<p>S’il existe une combinaison linéaire des colonnes nulle <span class="math notranslate nohighlight">\(\lambda_0\sum_{k=1}^K \lambda_k X_{k+1} + \mu_k X_{K+1+k}=0\)</span>, alors le polynôme trigonométrique</p>
<div class="math notranslate nohighlight">
\[g:t\mapsto \lambda_0+\sum_{k=1}^K \lambda_k \cos(2k\pi t) + \mu_k \sin(2k\pi t)\]</div>
<p>admet <span class="math notranslate nohighlight">\(2K+1\)</span> zéro sur <span class="math notranslate nohighlight">\([0,1]\)</span> et est donc la fonction nulle et les coefficients sont nuls. En particulier, le système est identifiable.</p>
<p>Calculons la matrice <span class="math notranslate nohighlight">\(X^tX\)</span> à l’aide de l’indication :</p>
<div class="math notranslate nohighlight">
\[\begin{split}X^tX= \begin{pmatrix}
                n &amp; \cdots &amp;\sum_{i=1}^n \cos (2k\pi \frac{i}{n}) &amp; \cdots &amp; \cdots &amp; \sum_{i=1}^n \sin (2k\pi \frac{i}{n})&amp;\cdots\\
                \vdots &amp; &amp;\vdots&amp;  &amp;  &amp; \vdots&amp;\\
                \sum_{i=1}^n \cos (2\tilde{k}\pi \frac{i}{n}) &amp; \cdots &amp;\sum_{i=1}^n \cos (2k\pi \frac{i}{n})\cos(2\tilde{k}\pi \frac{i}{n}) &amp; \cdots &amp; \cdots &amp; \sum_{i=1}^n \sin (2k\pi \frac{i}{n})\cos(2\tilde{k}\pi \frac{i}{n})&amp;\cdots \\
                \vdots &amp; &amp;\vdots&amp;  &amp;  &amp; \vdots&amp;\\
                \sum_{i=1}^n \sin (2\tilde{k}\pi \frac{i}{n}) &amp; \cdots &amp;\sum_{i=1}^n \cos (2k\pi \frac{i}{n})\sin(2\tilde{k}\pi \frac{i}{n}) &amp; \cdots &amp; \cdots &amp; \sum_{i=1}^n \sin (2k\pi \frac{i}{n})\sin(2\tilde{k}\pi \frac{i}{n})&amp;\cdots
            \end{pmatrix}\end{split}\]</div>
<p>Avec l’indication, pour <span class="math notranslate nohighlight">\(k\neq \tilde{k}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \cos (2k\pi \frac{i}{n})\sin(2\tilde{k}\pi \frac{i}{n})=\sum_{i=1}^n \frac{1}{2}Im(e^{2j\pi(\tilde{k}+k)\frac{i}{n}}-e^{2j\pi(\tilde{k}-k)\frac{i}{n}})=\frac{1}{2}Im(\frac{1-e^{2j\pi(\tilde{k}+k)}}{1- e^{2j\pi\frac{(\tilde{k}+k)}{n}}} -\frac{1-e^{2j\pi(\tilde{k}-k)}}{1- e^{2j\pi\frac{(\tilde{k}-k)}{n}}}=0\]</div>
<p>De la même manière, l’on prouve pour <span class="math notranslate nohighlight">\(k\neq \tilde{k}\)</span></p>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^n \cos (2k\pi \frac{i}{n})\cos(2\tilde{k}\pi \frac{i}{n}) =0\]</div>
<div class="math notranslate nohighlight">
\[ \sum_{i=1}^n \sin (2k\pi \frac{i}{n})\sin(2\tilde{k}\pi \frac{i}{n}) =0\]</div>
<p>et pour <span class="math notranslate nohighlight">\(k\neq 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \cos (2k\pi \frac{i}{n})=0 \]</div>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \sin (2k\pi \frac{i}{n})=0 \]</div>
<p>Enfin, pour <span class="math notranslate nohighlight">\(k=\tilde{k}\)</span>, l’on prouve que</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \cos (2k\pi \frac{i}{n})\cos(2\tilde{k}\pi \frac{i}{n}) = \frac{n}{2}\]</div>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^n \sin (2k\pi \frac{i}{n})\sin(2\tilde{k}\pi \frac{i}{n}) = \frac{n}{2}\]</div>
<p>Donc finalement <span class="math notranslate nohighlight">\(X^tX\)</span> est diagonale de la forme :</p>
<div class="math notranslate nohighlight">
\[\begin{split}X^tX= \begin{pmatrix}
            n&amp;0&amp;\cdots &amp;0\\
            0&amp;\frac{n}{2}&amp;\cdots &amp;0\\
            \vdots&amp;&amp;\ddots &amp;\vdots\\
            0&amp;&amp;\cdots&amp;\frac{n}{2}
        \end{pmatrix}\end{split}\]</div>
<p>Finalement,</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} =diag(\frac{2}{n},\frac{1}{n},...,\frac{1}{n}) *X^t Y\]</div>
<div class="math notranslate nohighlight">
\[\hat{\mu} =X*diag(\frac{2}{n},\frac{1}{n},...,\frac{1}{n}) *X^t Y\]</div>
<div class="math notranslate nohighlight">
\[\hat{f}=\hat{\beta}_1+\sum_{k=1}^K \hat{\beta}_{k+1}\cos(2k\pi t) +\hat{\beta}_{k+K+1}\sin(2k\pi t)\]</div>
</div>
<p><u>Overfitting et choix du modèle :</u></p>
<p>d) Calculer la somme des carrés de l’erreur quadratique renormalisé :</p>
<div class="math notranslate nohighlight">
\[r_n=\frac{\mathbb{E}(||Y-X\hat{\beta}||^2)}{n}\]</div>
<p>Pour K fixé, que se passe-t-il quand n tend vers l’infini ?</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution d)</p>
<p>Par définition, <span class="math notranslate nohighlight">\(Y-X\hat{\beta}= Y-\hat{Y}\)</span> est la projection de Y sur l’espace des colones de <span class="math notranslate nohighlight">\(X\)</span>. En particulier, d’après le théorème de Cochran, <span class="math notranslate nohighlight">\(||\frac{Y-X\hat{\beta}}{\sigma}||^2\sim \chi^2_{n-p}\)</span>.</p>
<p>Nous pouvons alors en déduire l’erreur quadratique renormalisé :</p>
<div class="math notranslate nohighlight">
\[r_n = \frac{\mathbb{E}(||Y-X\hat{\beta}||^2)}{n}=\frac{(n-p)\sigma}{n}\]</div>
<p>Pour K fixé, cette erreur moyenne tend vers <span class="math notranslate nohighlight">\(\sigma^2\)</span> : le bruit prédomine.</p>
</div>
<p>e) On suppose que p=n, donner alors la valeur de <span class="math notranslate nohighlight">\(r_n\)</span>. Que peut-on dire des valeurs de <span class="math notranslate nohighlight">\(\hat{f}\)</span> aux points <span class="math notranslate nohighlight">\(\frac{i}{n}\)</span> pour <span class="math notranslate nohighlight">\(i\in [\![1,n]\!]\)</span> ?</p>
<div class="dropdown admonition">
<p class="admonition-title">Solution e)</p>
<p>Nous avons alors que si <span class="math notranslate nohighlight">\(p=n\)</span>, alors <span class="math notranslate nohighlight">\(r_n=0\)</span>.</p>
<p>Nous avons trop de paramètre, et les valeurs de <span class="math notranslate nohighlight">\(\hat{f}\)</span> valent <span class="math notranslate nohighlight">\(Y_i\)</span>.</p>
</div>
<p>f) Nous prenons un polynôme trigonométrique avec coefficients non nuls que pour <span class="math notranslate nohighlight">\(k\leq 11\)</span> dans la décomposition ci-dessus. Nous faisons n = 101 observations et choisissions diverses valeurs de K. Nous obtenons la figure ci-dessous Quels phénomènes observe-t-on ? Quelle règle proposez-vous au vu des points précédents et de cette observation pour un bon ajustement ?</p>
<a class="reference internal image-reference" href="_images/Regression_fourrier.jpg"><img alt="Regression_fourrier" class="align-center" src="_images/Regression_fourrier.jpg" style="width: 400px;" /></a>
<div class="dropdown admonition">
<p class="admonition-title">Solution f)</p>
<p>Nous voyons une mauvaise estimation si p=3 (manque de paramètres dans ce cas) et si <span class="math notranslate nohighlight">\(p\approx n\)</span> (overfitting). En revanche, pour <span class="math notranslate nohighlight">\(p\approx \frac{n}{10}\)</span>, l’estimation est bonne.</p>
<p>Nous pouvons alors proposer la règle suivante : le nombre d’observations doit être supérieur à 10 fois le nombre de variables explicatives envisagé.</p>
</div>
</section>
</div>
<p>Lorsqu’on modélise un phénomène de la réalité, l’on cherche souvent à répondre à l’un des objectifs suivants :</p>
<ul class="simple">
<li><p><strong>Description</strong>, on recherche de façon exploratoire les liaisons entre une variable d’intérêt et d’autres variables potentiellement explicatives.</p></li>
<li><p><strong>Explication</strong>, on souhaite confirmer ou affiner une connaissance a priori du phénomène par l’estimation des paramètres et des tests appropriés.</p></li>
<li><p><strong>Prédiction</strong>, on souhaite exploiter le modèle pour prévoir des valeurs de la variable d’intérêt à partir de valeurs de variables explicatives. L’accent est alors mis sur la qualité des estimateurs et des variables explicatives, judicieusement sélectionnées.</p></li>
</ul>
<p>Mais de manière bien concrète, si le modèle que l’on utilise pour prédire un évènement met trop de temps à délivrer le résultat par des moyens de calculs à notre disposition, alors le modèle est inutile, voire néfaste (des unités de temps, de calculs et d’énergie aurait pu être investie ailleurs).\newline
La <em>parcimonie</em> est justement cet équilibre désirable entre précision du modèle et coûts du modèle (temps de calcul, nombre de variables nécessaires …).</p>
<p>Pour des méthodes d’estimations parcimonieuses que nous ne regarderons pas ici, l’on pourra s’intéresser à :</p>
<div class="hint dropdown admonition">
<p class="admonition-title">Méthode de Monte-Carlo</p>
<p>Pour l’aspect pratique de l’estimation de quantité sous forme d’espérance avec la méthode de Monte-Carlo  <span id="id1">[<a class="reference internal" href="biblio.html#id3" title="Jun Liu. Monte Carlo Strategies in Scientic Computing. Springer series in statistics, 02 2009. ISBN 0387763694. doi:10.1007/978-0-387-76371-2.">Liu, 2009</a>]</span>.</p>
</div>
<div class="hint dropdown admonition">
<p class="admonition-title">Estimation avec des chaines de Markov</p>
<p>Pour une deuxième méthode pratique d’estimation avec des chaines de Markov, et l’étude de leurs propriétés  <span id="id2">[<a class="reference internal" href="biblio.html#id4" title="David A. Levin, Yuval Peres, and Elizabeth L. Wilmer. Markov chains and mixing times. American Mathematical Society, 2006. URL: http://scholar.google.com/scholar.bib?q=info:3wf9IU94tyMJ:scholar.google.com/&amp;output=citation&amp;hl=en&amp;as_sdt=2000&amp;ct=citation&amp;cd=0.">Levin, Peres, and Wilmer, 2006</a>]</span>.</p>
</div>
<section id="algorithmes-de-selection-de-parametres-pour-la-regression-lineaire">
<h2><span class="section-number">6.1. </span>Algorithmes de sélection de paramètres pour la régression linéaire<a class="headerlink" href="#algorithmes-de-selection-de-parametres-pour-la-regression-lineaire" title="Link to this heading">#</a></h2>
<p>Rappelons que la régression linéaire, étudié dans <a class="reference internal" href="ch4sec4.html#linearregression"><span class="std std-ref">Estimation par régression linéaire</span></a>, consiste à trouver la “meilleure” (au sens de l’erreur quadratique) estimation affine des observations <span class="math notranslate nohighlight">\((y_i)_{i\in[\![1,n]\!]}\)</span> à partir des réalisations des paramètres explicatifs <span class="math notranslate nohighlight">\((x_{i,j})_{(i,j)\in[\![1,n]\!]\times[\![1,J]\!]}\)</span>, c’est-à-dire à trouver des paramètres <span class="math notranslate nohighlight">\((a_j)_{i\in[\![0,J]\!]}\)</span> réalisant le minimum de l’erreur quadratique :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{pmatrix}
    a_0\\\vdots\\a_n
\end{pmatrix} = arg\min\limits_{b_j}\left(\sum_{i=1}^n
(y_i-b_0-x_{i,1}b_1-x_{i,2}b_2-\cdots -x_{i,J}b_J)^2\right)\end{split}\]</div>
<p>Mais une fois fait ce calcul d’optimisation, nous pouvons nous demander si toutes les variables d’explications était vraiment nécessaires. Si nous sommes un peu plus souples dans les erreurs acceptées, pourrions-nous drastiquement diminuer la dimension du modèle (et donc les temps de calculs qui suivront) ? Pire, ne sommes-nous pas en train de faire un sur-apprentissage, valable uniquement pour les données obtenues, mais peu généralisable ?</p>
<p>Pour savoir ce que nous acceptons de sacrifier comme précision au profit de la diminution du nombre de paramètres, il va falloir nous fixer un critère. Pour que celui-ci fonctionne, il faut que ce critère soit grand quand le nombre de dimensions est grand ou quand l’erreur est grande. Nous verrons ici deux critères répondant à ces attentes.</p>
<div class="proof definition admonition" id="definition-1">
<p class="admonition-title"><span class="caption-number">Definition 6.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Soit <span class="math notranslate nohighlight">\((y_i)_{i\in[\![1,n]\!]}\)</span> une série d’observations, et <span class="math notranslate nohighlight">\((x_{i,j})_{(i,j)\in[\![1,n]\!]\times[\![1,J]\!]}\)</span> la série d’observations correspondantes des <span class="math notranslate nohighlight">\(J\)</span> variables explicatives.\newline</p>
<p>Pour <span class="math notranslate nohighlight">\(\Omega_r\subset [\![1,J]\!] \)</span> un sous-ensemble des explications de taille <span class="math notranslate nohighlight">\(r\leq J\)</span>, on appelle :</p>
<ul class="simple">
<li><p><em><span class="math notranslate nohighlight">\(C_p\)</span> de Mallows</em> de <span class="math notranslate nohighlight">\(\Omega_p\)</span> la quantité :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ C_p = (n-r-1)\frac{SE(\Omega_p)}{SE}- (n-2(r+1))\]</div>
<ul class="simple">
<li><p><em>Critère d’Akaike</em> de <span class="math notranslate nohighlight">\(\Omega_p\)</span> la quantité :</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ AIC(\Omega_r) = n \ln(\frac{SE(\Omega_p)}{n})+2(r+1)\]</div>
<p>où <span class="math notranslate nohighlight">\(SE\)</span> est l’erreur quadratique du système entier</p>
<div class="math notranslate nohighlight">
\[SE= \min\limits_{(b_j) \in \R^J}\left(\sum_{i=1}^n
(y_i-b_0-x_{i,1}b_1-x_{i,2}b_2-\cdots -x_{i,J}b_J)^2\right)= \min\limits_{b_j}\left(\sum_{i=1}^n
(y_i-b_0-\sum_{j=1}^Jx_{i,j}b_j)^2\right)\]</div>
<p>et <span class="math notranslate nohighlight">\(SE(\Omega_r)\)</span> est la somme des carrés d’erreur obtenue en ne retenant que les r variables de <span class="math notranslate nohighlight">\(\Omega_r\)</span> parmi les J initialement considérées :</p>
<div class="math notranslate nohighlight">
\[SE(\omega_r)= \min\limits_{(b_j) \in \R^r}\left(\sum_{i=1}^n
(y_i-b_0-x_{i,j_1}b_1-x_{i,j_2}b_2-\cdots -x_{i,j_r}b_J)^2\right)= \min\limits_{b_j}\left(\sum_{i=1}^n
(y_i-b_0-\sum_{j\in \Omega_r}x_{i,j}b_j)^2\right)\]</div>
</section>
</div><div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 6.1 </span></p>
<section class="remark-content" id="proof-content">
<p>Dans le cadre d’un modèle avec bruit gaussien centré et de variance connu égale à <span class="math notranslate nohighlight">\(\sigma Id\)</span>, les deux critères sont équivalant dans les résultats que donnera l’application des algorithmes de sélections. En revanche, ce n’est plus le cas lorsque l’on sort de ce cadre.</p>
</section>
</div><p>L’intuition derrière la construction des algorithmes de sélection est que plus la valeur du critère est petite et plus le modèle sélectionné est parcimonieux tout en restant efficace.</p>
<p>Nous voudrions donc trouver un modèle ne contenat que des variables d’importance. Mais dans le cas général et évidemment le plus courant en pratique, les variables
ne sont pas pré-ordonnées par importance. Il existe des algorithmes de sélection global (l’algorithme de Furnival et Wilson dit de “leaps and bound” par exemple, implémenté dans la plupart des librairies de statistique), mais ceux-ci ne sont souvent utilisables que pour un nombre de paramètres inférieur à 15.</p>
<p>Lorsque J est grand, il n’est pas raisonnable de penser explorer les <span class="math notranslate nohighlight">\(2^J\)</span> ensembles de variables explicatives afin de sélectionner le meilleur au sens de l’un des critères ci-dessus. Des algorithmes d’exploration pas-à-pas (algorithme de descente) existent. Ils se regroupent en trois types :</p>
<ul class="simple">
<li><p><strong>Sélection</strong> <em>(forward)</em> L’algorithme commence avec aucune variable. À chaque étape, on cherche à ajouter la variable qui diminue le plus le critère de sélection. L’algorithme s’arrête lorsque toutes les variables sont présentes ou lorsque le critère ne s’améliore plus.</p></li>
<li><p><strong>Élimination</strong> <em>(backward)</em> L’algorithme démarre avec toutes les variables. La variable dont la suppression conduit à la plus petite valeur du critère est alors retirée. La procédure s’arrête lorsque le critère ne décroit plus.</p></li>
<li><p><strong>Mixte</strong> <em>(stepwise)</em> Algorithme mélangeant les deux précédents.  Il ajoute une étape d’élimination après chaque étape de sélection, de façon à éliminer d’éventuelles variables devenues inutiles après l’introduction d’une nouvelle variable.</p></li>
</ul>
</section>
<section id="l-analyse-en-composante-principale-pour-diminuer-la-dimension">
<h2><span class="section-number">6.2. </span>L’analyse en composante principale pour diminuer la dimension<a class="headerlink" href="#l-analyse-en-composante-principale-pour-diminuer-la-dimension" title="Link to this heading">#</a></h2>
<p>Un autre cas de figure potentiel est si nous ne sommes pas assurés par la modélisation que nos variables explicatives soit bien indépendante et que nous ayons alors fait le meilleur choix possible dans leur description. Il est alors possible de faire une analyse en composante principale pour décrire les directions dans l’espace des variables explicative ayant la plus grande importance dans la variation de l’échantillon. Nous ne décrirons ici que le principe et la méthode numérique de calcul.</p>
<p>L’analyse en composante principale (ou ACP) consiste à trouver une ou plusieurs nouvelles variables explicatives, combinaison linéaire des anciennes, expliquant la plus grande partie de la variance \emph{via} une projection orthogonale.\newline
Considérons donc un ensemble de variables <span class="math notranslate nohighlight">\((X_i)_{i\in [\![1,n]\!]}\)</span> et donnons-nous <span class="math notranslate nohighlight">\(K\)</span> réalisations de l’échantillon, que nous écrivons de forme matricielle comme :</p>
<div class="math notranslate nohighlight">
\[\begin{split}\tilde{M}:=\begin{pmatrix}
    x_{1,1}&amp;\cdots &amp;x_{n,1}\\ \vdots &amp;&amp; \vdots\\ x_{1,K} &amp;\cdots &amp; x_{n,K}
\end{pmatrix}\end{split}\]</div>
<p>Et nous ne conserverons que la version recentrée de l’échantillon :</p>
<div class="math notranslate nohighlight">
\[\begin{split}{M}:=\begin{pmatrix}
    x_{1,1}-\overline{x}_1&amp;\ &amp;\cdots &amp;\ &amp;x_{n,1}-\overline{x}_n\\ \vdots &amp;&amp;&amp;&amp; \vdots\\ x_{1,K} - \overline{x}_1&amp;&amp;\cdots &amp;&amp; x_{n,K}-\overline{x}_n
\end{pmatrix}\end{split}\]</div>
<p>Si l’on se donne une direction <span class="math notranslate nohighlight">\(u\)</span>, les réalisations des  projection des variables explicatives <span class="math notranslate nohighlight">\((X_i)\)</span> sur la droite engendrée par u est le vecteur
<span class="math notranslate nohighlight">\(Mu\)</span>. Comme celui-ci est de moyenne empirique nulle, sa variance empirique est alors $<span class="math notranslate nohighlight">\( (Mu)^t Mu = ||Mu||_2^2\)</span>$</p>
<div class="proof definition admonition" id="definition-3">
<p class="admonition-title"><span class="caption-number">Definition 6.2 </span></p>
<section class="definition-content" id="proof-content">
<p>On appelle première composante principale la direction <span class="math notranslate nohighlight">\(u_1\)</span> maximisant après projection la variance empirique :</p>
<div class="math notranslate nohighlight">
\[u_1:= Arg\max\limits_{u \in \R^n, ||u||_2=1}\left(||Mu||_2^2 \right)\]</div>
<p>On définit par récurrence une <span class="math notranslate nohighlight">\(p^{ieme}\)</span> composante principale comme la direction <span class="math notranslate nohighlight">\(u_p\)</span> maximisant après projection la variance empirique restante  :</p>
<div class="math notranslate nohighlight">
\[u_p:= Arg\max\limits_{\underset{\forall i\in [\![1,p]\!], &lt;u_i,u&gt;=0}{u \in \R^n, ||u||_2=1}}\left(||Mu||_2^2 \right)\]</div>
<p>De manière équivalente, les composantes principales sont les vecteurs propres de la matrice <span class="math notranslate nohighlight">\(M^tM\)</span> si l’on a classé les valeurs propres par ordre croissant.</p>
<div class="proof dropdown admonition" id="proof">
<p>Proof. Estimation avec des chaines de Markov
L’équivalence découle du fait que <span class="math notranslate nohighlight">\(M^tM\)</span> est une matrice symétrique, et admet donc une diagonalisation en vecteurs propres orthogonaux avec des valeurs propres réelles (voir le théorème spectral dans n’importe quel livre d’analyse matricielle).</p>
<p>Notons <span class="math notranslate nohighlight">\(v_i\)</span> ces vecteurs propres, de valeurs propres <span class="math notranslate nohighlight">\(\mu_1\geq \mu_2...\geq \mu_n\)</span>.</p>
<p>Si l’on écrit <span class="math notranslate nohighlight">\(u=\sum_{i=p}^n \lambda_i v_i\)</span> avec <span class="math notranslate nohighlight">\(\sum_{i=p}^n \lambda_i^2=1\)</span>, alors on a que :</p>
<div class="math notranslate nohighlight">
\[ ||Mu||^2_2= &lt;Mu,Mu&gt;= \sum_{p\leq i,j\leq n} \lambda_i \lambda_j \mu_i\mu_j &lt;u_i,u_j&gt;\leq \mu_p^2 \sum_{p\leq i\leq n} \lambda_i^2 =\mu_p^2=||Mv_p||^2\]</div>
</div>
</section>
</div><p>Finalement, la question de l’ACP se ramène à un problème de diagonalisation de la matrice de covariance. Pour faciliter le traitement numérique, il est également possible d’utiliser la décomposition en valeur singulière de la matrice rectangulaire M.</p>
<p>Bien souvent, les données sont corrigées pour être de variance empirique 1 en divisant dans la matrice M les diverses colonnes par leur écart-type empirique (on parle alors d’ACP normée), ce qui ramène le calcul à une diagonalisation de la matrice de corrélation.</p>
<p>Une fois la décomposition en valeur singulière effectuée, la question du nombre de variables à conserver se pose souvent. Un certain nombre de règles empirique existent, basé sur les valeurs propres associés aux facteurs :</p>
<ul class="simple">
<li><p>Conserver les facteurs pour lesquels la valeur propre est supérieure à 1 (seuil de Kaiser, souvent considéré comme trop permissif).</p></li>
<li><p>Conserver les facteurs <span class="math notranslate nohighlight">\(u_p\)</span> pour lesquels la valeur propre est supérieure à <span class="math notranslate nohighlight">\(\sum_{i=p}^n \frac{1}{i}\)</span> (test des bâtons brisés à 5% correspondant à une distribution uniforme de la dispersion sur les axes).</p></li>
<li><p>Conserver les facteurs <span class="math notranslate nohighlight">\(u_p\)</span> pour lesquels la valeur propre est supérieure à <span class="math notranslate nohighlight">\(1-2\sqrt{\frac{p-1}{n-1}}\)</span> (test unilatéral de conformité à 5% de loi asymptotiquement normale).</p></li>
<li><p>Tracer les valeurs propres en fonction de leur numéro d’apparition, et ne garder que les valeurs propres avant la cassure nette de pente.</p></li>
<li><p>Tirer aléatoirement un échantillon de variables aléatoires indépendantes de loi normale centré-réduite, et comparer son ACP à celle des variables renormalisé centrée et réduite</p></li>
</ul>
<p>Dans tous les cas, il est pertinent de tracer cette courbe des valeurs propres en fonction de leur numéro d’apparition (appelé éboulis de valeur propre) et le critère correspondant.
Par exemple, si l’on cherche à extraire les groupes de questions pertinentes du test de rentrée de 2021 du parcours 3EA de l’ENSEEIHT, Toulouse-INP, l’on trouve le graphique suivant :</p>
<a class="reference internal image-reference" href="_images/Eboulis-2.png" id="fig-eboulis"><img alt="fig:Eboulis" class="align-center" id="fig-eboulis" src="_images/Eboulis-2.png" style="width: 600px;" /></a>
<p>Il faut ensuite savoir combien de recombinaison garder, suivant les critères, l’on gardera que le premier facteur (qui se trouve correspondre à la note totale), les 8 premiers facteurs (test de Kaiser et des bâtons brisés) ou encore tous les facteurs (test de conformité). Au vu du test aléatoire, il semble pertinent de ne conserver que la première composante. Ainsi, il n’y a pas de groupe de question expliquant mieux la dispersion entre individus que la note totale.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="ch5sec3.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.3. </span>Tester la loi d’observations</p>
      </div>
    </a>
    <a class="right-next"
       href="ch7intro.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span><i class="fas fa-book fa-fw"></i>  Annexes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithmes-de-selection-de-parametres-pour-la-regression-lineaire">6.1. Algorithmes de sélection de paramètres pour la régression linéaire</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#l-analyse-en-composante-principale-pour-diminuer-la-dimension">6.2. L’analyse en composante principale pour diminuer la dimension</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By P. Fraux
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  Lien vers la page principale du cours <a href="https://p-fraux.github.io/Cours-Mathematiques/intro.html"> Mathematiques de première année</a>.
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>